[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was developed by statisticians of the SCTO statistics and methodology platform."
  },
  {
    "objectID": "baselinetables.html",
    "href": "baselinetables.html",
    "title": "Make a baseline table",
    "section": "",
    "text": "Epidemiologic and clinical research papers often describe the study sample in a first, baseline table. Providing a baseline table is one of the recommendations of the CONSORT (Moher et al. (2012)) and STROBE (von Elm et al. (2007)) statements. If well-executed, it provides a rapid, objective, and coherent grasp of the data and can illuminate potential threats to internal and external validity (Hayes-Larson et al. (2019))."
  },
  {
    "objectID": "baselinetables.html#in-r",
    "href": "baselinetables.html#in-r",
    "title": "Make a baseline table",
    "section": "In R",
    "text": "In R\n\nData\n\nlibrary(dplyr)\nlibrary(Hmisc)\ndata(mtcars)\nmtcars$am_f <- factor(mtcars$am, 0:1, c(\"Manual\", \"Automatic\")) \nmtcars$vs_f <- factor(mtcars$vs, 0:1, c(\"V\", \"Straight\")) \nd <- mtcars %>%  select(mpg, cyl, am_f,vs_f)\nHmisc::label(d$mpg) <- \"Miles per gallon\"\nHmisc::label(d$cyl) <- \"Cylinders\"\nHmisc::label(d$am_f) <- \"Transmission\"\nHmisc::label(d$vs_f) <- \"Engine\"\n\n\n\nUsing gtsummary\n\n#Make a table stratify the table by transmission\nlibrary(gtsummary)\nd %>%  tbl_summary(by = am_f)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Manual, N = 191\n      Automatic, N = 131\n    \n  \n  \n    Miles per gallon\n17.3 (14.9, 19.2)\n22.8 (21.0, 30.4)\n    Cylinders\n\n\n    4\n3 (16%)\n8 (62%)\n    6\n4 (21%)\n3 (23%)\n    8\n12 (63%)\n2 (15%)\n    Engine\n\n\n    V\n12 (63%)\n6 (46%)\n    Straight\n7 (37%)\n7 (54%)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\n\n\nThis is the basic usage; defaults options may be customized.\n\nVariable types are automatically detected so that appropriate descriptive statistics are calculated.\nLabel attributes from the data set are automatically printed.\nMissing values are listed as “Unknown” in the table.\nVariable levels are indented and footnotes are added.\n\nOnce produced gtsummary tables can be converted to your favorite format (e.g. html/pdf/word). For more information see here)\n\n# declare cylinders as a continuous variable, for this variable calculate the mean and sd value, add an overall column, change the missing text\nd %>%\n        tbl_summary(by = am_f, \n                    type = list(cyl ~ 'continuous'),\n                    statistic = list(cyl ~ \"{mean} ({sd})\"),\n                    missing_text = \"Missing\") %>%\n        add_overall()\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Overall, N = 321\n      Manual, N = 191\n      Automatic, N = 131\n    \n  \n  \n    Miles per gallon\n19.2 (15.4, 22.8)\n17.3 (14.9, 19.2)\n22.8 (21.0, 30.4)\n    Cylinders\n6.19 (1.79)\n6.95 (1.54)\n5.08 (1.55)\n    Engine\n\n\n\n    V\n18 (56%)\n12 (63%)\n6 (46%)\n    Straight\n14 (44%)\n7 (37%)\n7 (54%)\n  \n  \n  \n    \n      1 Median (IQR); Mean (SD); n (%)\n    \n  \n\n\n\n\n\n\n\nAs before, the function detects variable type and uses an appropriate statistical test. If needed defaults may be customized.\n\nAdditional information\nFor more detailed tutorial and additional options see the very complete vignette and website\n\n\n\nUsing atable\n\nlibrary(atable)\ntable1=atable(d,\n       target_cols = c(\"mpg\" , \"cyl\" , \"vs_f\"),\n       group_col = \"am_f\",\n       format_to=\"Word\")\n\nor similar using the formula interface\n\ntable1=atable(mpg+cyl+ vs_f ~ am_f, d,\n              format_to=\"Word\")\ntable1\n\n\n\n                       Group    Manual Automatic      p stat   Effect Size (CI)\n1              Observations                                                    \n2           &emsp;   &emsp;         19        13   <NA> <NA>               <NA>\n3  Miles per gallon  &emsp;                                                    \n4          &emsp;  Mean (SD)  17 (3.8)  24 (6.2) 0.0019 0.64 -1.5 (-2.3; -0.65)\n5    &emsp;  valid (missing)    19 (0)    13 (0)   <NA> <NA>               <NA>\n6                 Cylinders                                                    \n7          &emsp;  Mean (SD) 6.9 (1.5) 5.1 (1.6)  0.013 0.48      1.2 (0.41; 2)\n8    &emsp;  valid (missing)    19 (0)    13 (0)   <NA> <NA>               <NA>\n9                    Engine                                                    \n10                 &emsp;  V  63% (12)   46% (6)   0.56 0.35       2 (0.38; 11)\n11          &emsp;  Straight   37% (7)   54% (7)   <NA> <NA>               <NA>\n12           &emsp;  missing    0% (0)    0% (0)   <NA> <NA>               <NA>\n\n\nBy default atable is printing p-values, test statistics as well as effect sizes with a 95% confidence interval. As stated above baseline tables are descriptive tables and should not contain this type of information. Don’t forget to remove the columns.\n\ntable1=table1%>%select(-\"p\",-\"stat\",-\"Effect Size (CI)\")\ntable1\n\n                   Group    Manual Automatic\n1          Observations                     \n2                               19        13\n3  Miles per gallon                         \n4              Mean (SD)  17 (3.8)  24 (6.2)\n5        valid (missing)    19 (0)    13 (0)\n6             Cylinders                     \n7              Mean (SD) 6.9 (1.5) 5.1 (1.6)\n8        valid (missing)    19 (0)    13 (0)\n9                Engine                     \n10                     V  63% (12)   46% (6)\n11              Straight   37% (7)   54% (7)\n12               missing    0% (0)    0% (0)\n\n\n\n\nThe table may also be split in strata. For example, we can decide to present separately the characteristics of car with a “V” or a “Straight” engine.\n\ntable1=atable(mpg+cyl  ~ am_f|vs_f , d,\n              format_to=\"Word\")\ntable1=table1%>%select(-\"p\",-\"stat\",-\"Effect Size (CI)\")\ntable1\n\n\n\nAs gtsummary, atable may be exported in different formats (e.g. LATEX, HTML, Word) and it is intended that some parts of atable can be altered by the user. For example the variable type may defined otherwise (for more details see Ströbel (2019) as well as the package vignette. An other informative vignette can be found by typing the following command in R:\n\nvignette(\"modifying\", package = \"atable\")"
  },
  {
    "objectID": "baselinetables.html#instalation",
    "href": "baselinetables.html#instalation",
    "title": "Make a baseline table",
    "section": "Instalation",
    "text": "Instalation\n\n#In order to install btable from github the github-package is required:\nnet install github, from(\"https://haghish.github.io/github/\")\n#You can then install the development version of btable with:\ngithub install CTU-Bern/btable"
  },
  {
    "objectID": "baselinetables.html#example",
    "href": "baselinetables.html#example",
    "title": "Make a baseline table",
    "section": "Example",
    "text": "Example\n\n# load example dataset\nsysuse auto2\n# generate table\nbtable price mpg rep78 headroom, by(foreign) saving(\"excars\") denom(nonmiss)\n# format table (default formatting, removing the effect measures and P-values)\nbtable_format using \"excars\", clear drop(effect test)\n\n\n\nThe formatting option can be modified. For example we can decide we may want to\n\npresent the median and lower and upper quartiles instead of the mean and standard deviation\nremove the overall column, and the information column\n\n\n#If we want to display median [lq, up] for all the continuous variables\nbtable_format using \"excars\", clear descriptive(conti median [lq, uq]) drop(effect test total info)\n#If we want to display mean (sd) for the mpg variable and median [lq, up] for all the other continuous variables\nbtable_format using \"excars\", clear desc(conti median [lq, uq] mpg mean (sd)) drop(effect test total info)\n\n\n\n\n\n Domestic (N = 52)Foreign (N = 22)mean (sd), median [lq, uq] or n (%)mean (sd), median [lq, uq] or n (%)Price4783 [4184, 6234]5759 [4499, 7140]Mileage (mpg)20 (4.7)25 (6.6)Repair Record 1978Poor2 (4.2%)0 (0.00%)Fair8 (17%)0 (0.00%)Average27 (56%)3 (14%)Good9 (19%)9 (43%)Excellent2 (4.2%)9 (43%)Headroom (in.)3.5 [2.3, 4.0]2.5 [2.5, 3.0]"
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "blog",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post-with-code/index.html",
    "href": "blog/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Courses are run in the following approximate locations:"
  },
  {
    "objectID": "DataSharing.html",
    "href": "DataSharing.html",
    "title": "Data sharing",
    "section": "",
    "text": "A summary of the document here\nhttps://www.sctoplatforms.ch/en/publications/sharing-of-data-from-clinical-research-projects-guidance-from-the-sctors-clinical-trial-unit-network-143.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Topics are split according to the target audience, clinicians or programmers.\n\n\n\n\n\n\nImportant\n\n\n\nThis is site is currently only for demonstration purposes. It is more conceptual than anything else.\nContent will be modified without warning or the site may be removed without notice.\nWe make no guarantees on the accuracy of the information presented.\n\n\nThe official SCTO platforms website can be found here."
  },
  {
    "objectID": "ReportingGuidelines.html",
    "href": "ReportingGuidelines.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "Because different project types require different things to be reports, there are many guidelines for reporting different types of projects.\n\nRandomised trials\n\nCONSORT - guidelines for publishing RCTs\n\nChecklist\nStatement\nFlow diagram\nDISCOURAGES USE OF P-VALUES/CONFIDENCE INTERVALS/STANDARD ERRORS IN DESCRIPTIVE TABLES (e.g. Table 1) - POSSIBLY USEFUL TO REFUTE REVIEWER REQUESTS FOR THEM (see also the Datamethods reference collection on “common myths”)\nExtension for adaptive designs\n\n(checklist in the supplementary materials)\n\n\n\n\n\nObservational studies\n\nSTROBE - guidelines for reporting of observational studies\n\nSTROBE website\nSTROBE checklists\nThe STROBE statement itself has been published in many journals (see here)\n\n\n\n\nSystematic reviews and meta analyses\n\nPRISMA - guidelines for transparent reporting of systematic reviews\n\nHomepage\nChecklist\nExtensions to the original statement\nLink registration details page where registrations are actually made on PROSPERO\nNote that systematic reviews/metaanalyses are sometimes/often rejected by journals for not having been registered\n\n\n\n\nPredictive models\n\nTRIPOD - guidelines for reporting of predictive/prognostic models (validation or derivation)\n\nMoons et al 2015\nMoons et al 2012\nTRIPOD checklists\nAlternate site for derivation/validation checklist\n\n\nNot a reporting guideline per se, but a method of assessing risk of bias and applicability of prediction model studies - PROBAST\n\nPROBAST paper\nfurther explanations and elaboration of PROBAST\nuseful along side TRIPOD perhaps?\nuse of PROBAST to assess ML models in ocology. Long story short, most models are high risk > 123 (81%, 95% CI: 73.8 to 86.4) developed models and 19 (51%, 95% CI: 35.1 to 67.3) validated models were at high risk of bias due to their analysis, mostly due to shortcomings in the analysis including insufficient sample size and split-sample internal validation\n\n\n\nOthers\n\nLatent trajectory studies (GRoLTS)\n\nThe GRoLTS-Checklist: Guidelines for Reporting on Latent Trajectory Studies\n\n\nThe EQUATOR Network also has links to many other guidelines (SPIRIT, CARE, AGREE, …)\nThe COMET Initiative has a searchable list of standardised outcome sets for diseases, conditions etc."
  },
  {
    "objectID": "SampleSizeCalculation.html",
    "href": "SampleSizeCalculation.html",
    "title": "Sample size calculation",
    "section": "",
    "text": "Quiz\nHere is a short quiz to check your understanding…\n\n\nQuestion 1: In a court room, the defendant is incorrectly sentenced to prison. This is an example of…\n\nA. A type 1 error\nB. A type 2 error\n\n\n\nAnswer\n\nA is the correct answer. The defendant is innocent, so sentencing them to prison is wrong, a false positive.\n\n\n\nQuestion 2: Type 2 errors are controlled by\n\nA. Test significance\nB. Larger sample size\nC. Power\nD. Smaller variation between participants\n\n\n\nAnswer\n\nB or C - a larger sample increases power which reduces the risk of a type 2 error\n\n\n\nQuestion 3: You suspect that there is a difference between treatments and want to quantify that difference to within a certain margin. What paradigm are you in?\n\nA. Null-hypothesis testing\nB. Bayesian\nC. Precision\nD. Frequentist\n\n\n\nAnswer\n\nC - you’re not testing a difference, just trying the estimate the quantity to within a given limit\n\n\n\nQuestion 4: Continuing from the Question 3. Suppose instead that you want to test if one treatment has a higher mean value of an outcome than the other treatment. What information would the statistician need to perform a sample size calculation? Check all that might apply.\n\nA. The cost of determining the outcome\nB. The mean value of the outcome in the relevant population\nC. An estimate of the variability within the population\nD. An estimate of the difference you expect to see between treatments\nE. The number of individuals with the condition that pass through your institution per year\n\n\n\nAnswer\n\nB, C, and D are all important. E can also be useful for approximating trial duration.\n\n\n\n\nExample(s)\nHere we present a couple of very short examples of how sample size calculations can be performed, although we do recommend discussing your project with a statistician.\n\nThe NHST framework\nSuppose we want to estimate the mean difference between two groups (intervention vs placebo) and would like to be able to test it against “no difference”. We have to define which difference will be clinically relevant to test. Let assume that a difference of 20 between both groups is clinically relevant. We also need to guess in an educated way the standard deviation (SD) of the outcome in the groups. Let assume a SD of 12 in each group. We also have to set the power we want to achieve and the probability of type I error (alpha) we will allowed. In this example a power of 90% with a two-sided alpha of 5% can be reached with a sample size of 18, i.e. 9 participants in each group, to test if this difference is equal to null.\nParameters to assume or set:\n\nmean difference (delta)\nstandard deviations\npower\nprobability of type I error (alpha)\n\n\npower.t.test(delta = 20, sd = 12, sig.level = 0.05, power = .9,\n             type = \"two.sample\",\n             alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 8.649245\n          delta = 20\n             sd = 12\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf the difference between both groups is smaller the requested sample size to reach the same power with all the other parameters as above will be larger. Similarly, increasing the larger variation around the mean (larger standard deviation) also increases the sample size.\n\npower.t.test(delta = 10, sd = 12, sig.level = 0.05, power = .9,\n             type = \"two.sample\",\n             alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 31.25372\n          delta = 10\n             sd = 12\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nMultiple plausible scenario can be investigated, by varying some parameters. The sse package (Fabbro (2021)), provides a mechanism to create multiple scenarios and examine power in each, together with functions to aid reporting. sse is very flexible, but does require some programming.\n\nlibrary(sse)\n\n## defining the range of n and theta to be evaluated\npsi <- powPar(\n  # SD values\n  theta = seq(from = 5, to = 20, by = 1),\n  # sample sizes\n  n = seq(from = 5, to = 50, by = 2),\n  # group means\n  muA = 0,\n  muB = 20)\n\n## define a function to return the power in each scenario\npowFun <- function(psi){\n  power.t.test(n = n(psi)/2,\n               delta = pp(psi, \"muA\") - pp(psi, \"muB\"),\n               sd = theta(psi)\n  )$power\n}\n\n## evaluate the power-function for all combinations of n and theta\ncalc <- powCalc(psi, powFun)\n\n## choose one particular example at theta of 1 and power of 0.9\npow <- powEx(calc, theta = 12, power = 0.9)\n\n## drawing the power plot with 3 contour lines\nplot(pow,\n     xlab = \"Standard Deviation\",\n     ylab = \"Total Sample Size\",\n     at = c(0.85, 0.9, 0.95))\n\n\n\n\nFor additional details, see the sse package documentation on CRAN.\n\n\nThe precision based framework\nSuppose we want to estimate the proportion of deaths within 30 days of a myocardial infarction (MI). Law, Watt, and Wald (2002) estimated that 36% (31% - 40%) of patients died within 30 days of an MI. If we want to replicate their study, we can estimate the number of participants that would be necessary to achieve a CI which is 9% wide. Members of the SCTO Statistics and Methodology platform created an R package specifically for this problem called presize (Haynes et al. (2021)). It is available on CRAN or as an easy to use shiny application.\nThe relevant function in presize is the prec_prop function. Plugging in the numbers from above, we can see that we require 434 participants (column n) to yield a confidence interval from 31.6% (lwr) to 40.6% (upr).\n\nlibrary(presize)\nprec_prop(p = 0.36, conf.width = 0.09)\n\n\n     sample size for a proportion with Wilson confidence interval. \n\n     p      padj        n conf.width conf.level       lwr       upr\n1 0.36 0.3612296 433.5577       0.09       0.95 0.3162296 0.4062296\n\nNOTE: padj is the adjusted proportion, from which the ci is calculated.\n\n\nIf we know that we can only afford to follow 300 participants, we can see what the confidence interval would be in that case too, via the n argument (instead of cond.width):\n\nprec_prop(p = 0.36, n = 300)\n\n\n     precision for a proportion with Wilson confidence interval. \n\n     p    padj   n conf.width conf.level       lwr       upr\n1 0.36 0.36177 300  0.1080014       0.95 0.3077693 0.4157707\n\nNOTE: padj is the adjusted proportion, from which the ci is calculated.\n\n\nIt’s also possible to check multiple scenarios, by passing multiple values to a parameter. Below we vary the number of participants (what CI width is possible to determine with that number of participants?):\n\nprec_prop(p = .36, n = c(300, 350, 400, 450, 450))\n\n\n     precision for a proportion with Wilson confidence interval. \n\n     p      padj   n conf.width conf.level       lwr       upr\n1 0.36 0.3617700 300 0.10800136       0.95 0.3077693 0.4157707\n2 0.36 0.3615199 350 0.10007265       0.95 0.3114836 0.4115562\n3 0.36 0.3613317 400 0.09366763       0.95 0.3144979 0.4081655\n4 0.36 0.3611850 450 0.08835346       0.95 0.3170083 0.4053617\n5 0.36 0.3611850 450 0.08835346       0.95 0.3170083 0.4053617\n\nNOTE: padj is the adjusted proportion, from which the ci is calculated.\n\n\nThe results can also be plotted easily, which is particularly useful when running multiple scenarios.\n\nlibrary(ggplot2)  # for plotting\nlibrary(magrittr) # for 'piping'\n\nprec_prop(p = .36, n = c(300, 350, 400, 450, 500)) %>% \n  as.data.frame() %>% \n  ggplot(aes(x = n, y = conf.width)) +\n  geom_line() +\n  labs(x = \"N participants\", \n       y = \"CI width\")\n\n\n\n\nFor additional details and examples, see the presize website.\n\n\n\n\n\n\n\nConclusion\nSample size calculation is a very important part of the study planning process. To make the most of your meeting with the statistician, have the answers to the questions mentioned above.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFabbro, Thomas. 2021. Sse: Sample Size Estimation. https://CRAN.R-project.org/package=sse.\n\n\nHaynes, Alan G., Armando Lenz, Odile Stalder, and Andreas Limacher. 2021. “‘Presize‘: An r-Package for Precision-Based Sample Size Calculation in Clinical Research.” Journal of Open Source Software 6 (60): 3118. https://doi.org/10.21105/joss.03118.\n\n\nLaw, Malcolm R., Hilary C. Watt, and Nicholas J. Wald. 2002. “The Underlying Risk of Death After Myocardial Infarction in the Absence of Treatment.” Archives of Internal Medicine 162 (21): 2405–10. https://doi.org/10.1001/archinte.162.21.2405."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software packages",
    "section": "",
    "text": "Working with statistical software is the daily business of our statisticians. Most software languages allow their users to create their own packages of custom functions to reduce errors in repeated tasks. The software used by SCTO statisticians, primarily R and Stata, are no different in this respect. This page provides an overview of some."
  },
  {
    "objectID": "software.html#presize---precision-based-sample-size-estimation",
    "href": "software.html#presize---precision-based-sample-size-estimation",
    "title": "Software packages",
    "section": "presize - precision based sample size estimation",
    "text": "presize - precision based sample size estimation\n    \npresize is an R package for precision based sample size calculation. It provides a large number of methods for estimating the number of samples required to gain a confidence interval of a given width, or the width that might be expected with a given sample size.\n\n\nExample\n\nAssuming that we want to estimate the confidence interval (CI) around the sensitivity of a test, but we’re not sure of the sensitivity, we can estimate the CI width in a range of scenarios as follows.\n\n\nCode\nlibrary(presize)\n# set up a range of scenarios\nscenarios <- expand.grid(sens = seq(.5, .95, .1),\n                         prev = seq(.1, .2, .04),\n                         ntot = c(250, 350))\n# calculate the CI width at ntot individuals with prev prevalence of event\nscenario_data <- prec_sens(sens = scenarios$sens, \n                           prev = scenarios$prev, \n                           ntot = scenarios$ntot, \n                           method = \"wilson\")\n# plot the scenarios with ggplot2\nscenario_df <- as.data.frame(scenario_data)\nlibrary(ggplot2)\nggplot(scenario_df, \n       aes(x = sens, \n           y = conf.width, \n           # convert colour to factor for distinct colours rather than a continuum\n           col = as.factor(prev), \n           group = prev)) +\n  geom_line() +\n  labs(x = \"Sensitivity\", y = \"CI width\", col = \"Prevalence\") + \n  facet_wrap(vars(ntot))\n\n\n\n\n\n\nFor ease of use, presize also includes a shiny app for point-and-click use, which is also available on the internet.\n\n\nInstallation\n\npresize can be installed in R via the following methods:\n# from CRAN (the stable version)\ninstall.packages(\"presize\")\n\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"presize\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#sse---sample-size-estimation",
    "href": "software.html#sse---sample-size-estimation",
    "title": "Software packages",
    "section": "sse - sample size estimation",
    "text": "sse - sample size estimation\n   \nsse is another R package for sample size calculation that has been in use at CTU Basel for many years. It’s approach is very general, allowing a wide range of scenarios to be assessed rapidly.\n\n\nExample\n\nIf we want to find the sample size for comparing two means, using a range of standard deviations we can do the following.\n\n\nCode\nlibrary(sse)\n## defining the range of n and theta to be evaluated\npsi <- powPar(\n  # SD values\n  theta = seq(from = 5, to = 20, by = 1),\n  # sample sizes\n  n = seq(from = 5, to = 50, by = 2),\n  # group means\n  muA = 0,\n  muB = 20)\n## define a function to return the power in each scenario\npowFun <- function(psi){\n  power.t.test(n = n(psi)/2,\n               delta = pp(psi, \"muA\") - pp(psi, \"muB\"),\n               sd = theta(psi)\n  )$power\n}\n## evaluate the power-function for all combinations of n and theta\ncalc <- powCalc(psi, powFun)\n\n\nAssuming that a standard deviation of 12 is appropriate in this case, and we want a power of 90%, we can plot the power curve:\n\n## choose one particular example at theta of 1 and power of 0.9\npow <- powEx(calc, theta = 12, power = 0.9)\n## drawing the power plot with 3 contour lines\nplot(pow,\n     xlab = \"Standard Deviation\",\n     ylab = \"Total Sample Size\",\n     at = c(0.85, 0.9, 0.95))\n\n\n\n\n\n\n\nInstallation\n\nsse can be installed in R via the following methods:\n# from CRAN (the stable version)\ninstall.packages(\"sse\")\n\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"sse\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#sts_graph_landmark---landmark-analysis-graphs",
    "href": "software.html#sts_graph_landmark---landmark-analysis-graphs",
    "title": "Software packages",
    "section": "sts_graph_landmark - landmark analysis graphs",
    "text": "sts_graph_landmark - landmark analysis graphs\n \nsts_graph_landmark is a Stata program to create landmark analysis Kaplan-Meier curves, complete with risk table.\n\n\nExample\n\nUsing sts_graph_landmark is consistent with the other sts_* programs in Stata. The dataset should be stset and then sts_graph_landmark can be called specifying the landmark time in at.\n\n\nCode\n# load example dataset (note: this example is nonsensical and only for graphing purposes)\nwebuse stan3, clear\n# set data as survival data\nstset t1, failure(died) id(id)\n# label treatment arms \nlabel define posttran_l 0 \"prior transplantation\" 1 \"after transplantation\"\nlabel value posttran posttran_l\n# create landmark plot and table \nsts_graph_landmark, at(200) by(posttran) risktable\n\n\n\n\n\n\nInstallation\n\nIt can be installed from github:\nnet install github, from(\"https://haghish.github.io/github/\")\ngithub install CTU-Bern/sts_graph_landmark"
  },
  {
    "objectID": "software.html#secutrialr---import-secutrial-datasets-to-r",
    "href": "software.html#secutrialr---import-secutrial-datasets-to-r",
    "title": "Software packages",
    "section": "secuTrialR - import secuTrial datasets to R",
    "text": "secuTrialR - import secuTrial datasets to R\n    \n\nsecuTrial datasets consist of a lot of files and it can be difficult to get to grips with them. secuTrialR tries to reduce the burden by providing a method to import and format (e.g. adding labels to variables) and explore data.\n\n\nExample\n\nData can be read into R using read_secuTrial. The visit_structure function gives an idea of which forms are required at which visit. plot_recruitment is for plotting trial recruitment.\n\n\nCode\nlibrary(secuTrialR)\n# prepare path to example export\nexport_location <- system.file(\"extdata\", \"sT_exports\", \"snames\",\n                               \"s_export_CSV-xls_CTU05_short_miss_en_utf8.zip\",\n                               package = \"secuTrialR\")\n# read all export data\nsT_export <- read_secuTrial(data_dir = export_location)\nplot(visit_structure(sT_export))\nplot_recruitment(sT_export)\n\n\n\n\n\n\n\n\n\n\n\n\n\nsecuTrialR was developed by the data management platform with substantial input from members of the statistics and methodology platform.\n\n\nInstallation\n\nsecuTrialR can be installed in R via the following methods:\n# from CRAN (the stable version)\ninstall.packages(\"secuTrialR\")\n\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"secuTrialR\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#accrualplot---simple-creation-of-accrual-plots",
    "href": "software.html#accrualplot---simple-creation-of-accrual-plots",
    "title": "Software packages",
    "section": "accrualPlot - simple creation of accrual plots",
    "text": "accrualPlot - simple creation of accrual plots\n   \naccrualPlot is an R package for summarizing trial recruitment data. With relatively little code, it is possible to create various plots and tables useful for recruitment reports, as well as predict the end of recruitment based on the recruitment to date.\n\n\nExample\n\naccrualPlot includes a simulated dataset of participants recruited into a trial in one of three sites. The accrual_create_df function is used to define the properties of the sites (e.g. start dates if that differs from the first participants recruitment date). The plot and summary functions can then be used to plot or tabulate the data. The data can be plot using either base graphics or ggplot2.\n\n\nCode\nlibrary(accrualPlot)\ndata(accrualdemo)\ndf <- accrual_create_df(accrualdemo$date, by = accrualdemo$site)\n# cumulative recruitment\nplot(df, which = \"cum\", engine = \"ggplot2\")\n# absolute recruitment (daily/weekly/monthly)\nplot(df, which = \"abs\", engine = \"ggplot2\")\n# predict end date\nplot(df, which = \"pred\", target = 300, engine = \"ggplot2\")\n# summary table\nlibrary(gt)\ngt(summary(df)) %>% \n  tab_options(column_labels.hidden = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n    Center\nFirst participant in\nMonths accruing\nParticipants accrued\nAccrual rate (per month)\n    Site 1\n09Jul2020\n3\n141\n45.98\n    Site 2\n20Jul2020\n3\n88\n32.59\n    Site 3\n04Sep2020\n1\n21\n18.00\n    Overall\n09Jul2020\n3\n250\n81.52\n  \n  \n  \n\n\n\n\n\n\n\n\n\nInstallation\n\naccrualPlot can be installed in R via the following methods:\n# from CRAN (the stable version)\ninstall.packages(\"accrualPlot\")\n\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"accrualPlot\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#btable---create-baseline-tables-in-stata",
    "href": "software.html#btable---create-baseline-tables-in-stata",
    "title": "Software packages",
    "section": "btable - create baseline tables in Stata",
    "text": "btable - create baseline tables in Stata\n \nCreating baseline tables is a repetitive task. Each paper needs one. btable provides a powerful approach to creating them. See the making baseline tables article for an example. More information on btable can be found here.\n\n\nInstallation\n\nbtable can be installed in Stata via the following method:\nnet install github, from(\"https://haghish.github.io/github/\")\ngithub install CTU-Bern/btable"
  },
  {
    "objectID": "software.html#btabler---format-tables-for-latex-reports",
    "href": "software.html#btabler---format-tables-for-latex-reports",
    "title": "Software packages",
    "section": "btabler - format tables for LaTeX reports",
    "text": "btabler - format tables for LaTeX reports\n  \nbtabler adds additional functionality to the xtable package such as merging column headers for use in tables for LaTeX. It was originally developed as an easy way to put tables generated by `btable` into LaTeX reports, hence the similarity in names.\n\n\nExample\n\n\nlibrary(btabler)\ndf <- data.frame(name = c(\"\", \"\", \"Row 1\", \"Row2\"),\n                 out_t = c(\"Total\", \"mean (sd)\", \"t1\", \"t1\"),\n                 out_1 = c(\"Group 1\", \"mean (sd)\", \"g11\", \"g12\"),\n                 out_2 = c(\"Group 2\", \"mean (sd)\", \"g21\", \"g22\"))\nbtable(df, nhead = 2, nfoot = 0, caption = \"Table1\")\n\nWhich will look like this in after LaTeX has created your PDF:\n\n\n\n\nInstallation\n\nbtabler can be installed in R via the following method:\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"btabler\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#hsar---create-reproducible-hospital-service-areas-in-r",
    "href": "software.html#hsar---create-reproducible-hospital-service-areas-in-r",
    "title": "Software packages",
    "section": "HSAr - create reproducible hospital service areas in R",
    "text": "HSAr - create reproducible hospital service areas in R\n   \nHospital service areas can be useful for hospital planning, but their main use is in small area research. They are traditionally made largely by hand, by assigning each location to the hospital where most residents go and then iteratively moving locations until two main criteria are fulfilled - a HSA should not have detached islands, and at least 50% of it’s hospitalizations should stay there. The iterative steps are largely manual subjective work. As such the reproducibility of HSA creation is poor.\n`HSAr` provides an automated algorithm for creating HSAs by starting at the hospital and building the HSA around it until all regions in the provided shapefile are assigned to a HSA.\n`HSAr` was developed as part of national research programme 74, smarter health care.\n\n\nExample\n\n\n\n\nInstallation\n\nHSAr can be installed in R via the following method:\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"btabler\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#kpitools---tools-to-assist-with-risk-based-management-kpis",
    "href": "software.html#kpitools---tools-to-assist-with-risk-based-management-kpis",
    "title": "Software packages",
    "section": "kpitools - tools to assist with risk based management KPIs",
    "text": "kpitools - tools to assist with risk based management KPIs\n  \nIt is not enough to simply run a trial. ICH GCP E5 also requires risk based monitoring to be performed. kpitools provides a set of summary functions and a standardized format for presenting the key performance indicators (KPIs) that are typically defined for risk based monitoring strategies.\n\n\nExample\n\nIt could be that we believe that time of day might be an indicator of data fabrication because it’s not possible that participants are randomised at certain times of the day. The fab_tod function can help depict that..\n\nlibrary(kpitools)\n\nLade nötiges Paket: dplyr\n\n\n\nAttache Paket: 'dplyr'\n\n\nDas folgende Objekt ist maskiert 'package:sse':\n\n    n\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLade nötiges Paket: magrittr\n\n\nLade nötiges Paket: purrr\n\n\n\nAttache Paket: 'purrr'\n\n\nDas folgende Objekt ist maskiert 'package:magrittr':\n\n    set_names\n\n\nLade nötiges Paket: rlang\n\n\n\nAttache Paket: 'rlang'\n\n\nDie folgenden Objekte sind maskiert von 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\n\nDas folgende Objekt ist maskiert 'package:magrittr':\n\n    set_names\n\nset.seed(12345)\ndat <- data.frame(\n  x = lubridate::ymd_h(\"2020-05-01 13\") + 60^2*rnorm(40, 0, 3),\n  mean = rnorm(40, 56, 20),\n  by = sample(1:4, 40, prob = c(.2,.25,.4,.4), replace = TRUE)\n)\ndat %>% kpi(\"mean\", kpi_fn_mean, by = \"by\") %>% plot\n\n$by\n\n\n\n\ndat %>% fab_tod(\"x\")\n\n\n\n\n\n\n\nInstallation\n\nkpitools can be installed in R via the following method:\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"kpitools\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#redcaptools---a-package-for-working-with-redcap-data-in-r",
    "href": "software.html#redcaptools---a-package-for-working-with-redcap-data-in-r",
    "title": "Software packages",
    "section": "redcaptools - a package for working with REDCap data in R",
    "text": "redcaptools - a package for working with REDCap data in R\n  \nREDCap is a popular database for clinical research, used by many of the CTUs in Switzerland. One aggravation with REDCap data exports is that the data is in one file which can contain a lot of empty cells when more complicated database designs are used. redcaptools has tools to automatically pull the database apart into forms for easier use. Similar to secuTrialR, it also labels variables, and prepares date and factor variables. The function is primarily for interacting with REDCap via the Application Programming Interface (API), allowing easy scripted exports.\n\n\nExample\n\nBy supplying the API token generated by REDCap, together with the APIs URL, the redcap_export_byform function can be used to export all data from the database by form. Each form is returned as an element of a list.\n\nlibrary(redcaptools)\ntoken <- \"some-long-string-provided-by-redcap\"\nurl <- \"https://link.to.redcap/api/\"\ndat <- redcap_export_byform(token, url)\n\nThe ‘normal’ format can be exported via the redcap_export_tbl function:\n\nrecord_data <- redcap_export_tbl(token, url, \"record\")\nmeta <- redcap_export_tbl(token, url, \"metadata\")\n\nThis function can also be used to export various other API endpoints (e.g. various types of metadata etc, specific forms).\nThe data can then be formatted by using the metadata and the rc_prep function\n\nprepped <- rc_prep(dat, meta)\n\n\n\n\nInstallation\n\nredcaptools can be installed in R via the following method:\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"redcaptools\", repos = \"https://ctu-bern.r-universe.dev/\")"
  },
  {
    "objectID": "software.html#stata_secutrial---some-stata-code-to-do-data-import-and-preparation-of-secutrial-datasets",
    "href": "software.html#stata_secutrial---some-stata-code-to-do-data-import-and-preparation-of-secutrial-datasets",
    "title": "Software packages",
    "section": "stata_secutrial - some Stata code to do data import and preparation of secuTrial datasets",
    "text": "stata_secutrial - some Stata code to do data import and preparation of secuTrial datasets\n \nSimilar to secuTrialR above, stata_secutrial provides Stata code to read and prepare secuTrial exports in Stata. It labels variables, formats date variables, add labels to categorical variables etc, saving each form as a dta file for your further use.\n\n\nExample\n\nAssuming certain folders and globals have been prepared in advance (see GitHub for further information), using stata_secutrial may be as simple as entering\ndo SecuTrial_zip_data_import\ninto Stata and then navigating to your download when prompted.\n\n\n\nInstallation\n\nAs stata_secutrial is just code rather than a package, you can copy the files from GitHub and use then in you project. Towards the top of the GitHub page is a green code button. Click that and choose download ZIP. You can then unzip the files to your working directory."
  },
  {
    "objectID": "software.html#swissasr---simplified-annual-safety-reports-with-r",
    "href": "software.html#swissasr---simplified-annual-safety-reports-with-r",
    "title": "Software packages",
    "section": "SwissASR - simplified annual safety reports with R",
    "text": "SwissASR - simplified annual safety reports with R\n  \nEthics and regulators often require annual safety reports. SwissASR provides a relatively easy way to produce annual safety reports according to the current template available on the SwissMedic(?) website. The function returns a word file with the safety data completed based on the data provided to it. Minimal additional details should then be added by the study team or principal investigator.\n\n\nExample\n\n\n\n\nInstallation\n\nSwissASR can be installed in R via the following method:\n# from CTU Bern's package universe (the development version)\ninstall.packages(\"SwissASR\", repos = \"https://ctu-bern.r-universe.dev/\")"
  }
]